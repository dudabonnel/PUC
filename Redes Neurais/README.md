# üß† Redes Neurais 

## Informa√ß√µes sobre a disciplina

<table>
  <tr>
    <th>CURSO</th>
    <td>Ci√™ncia de Dados e Intelig√™ncia Artificial</td>
  </tr>
  <tr>
    <th>DISCIPLINA</th>
    <td>Consultoria Especializada de Apoio ao Projeto Integrado: Aprendizagem 
de M√°quinas</td>
  </tr>
    <tr>
    <th>PER√çODO</th>
    <td>5¬∫</td>
    </tr>
    <tr>
    <th>CARGA HOR√ÅRIA</th>
    <td>72 h.a</td>
  </tr>
    <tr>
    <th>SEMESTRE / ANO</th>
    <td>1¬∞ Semestre de 2023</td>
  </tr>
    <tr>
    <th>PROFESSOR</th>
    <td>Rooney R. A. Coelho</td>
  </tr>
</table>

## Ementa

Estudo  e  aplica√ß√£o  do  aprendizado  supervisionado:  algoritmos  para  classifica√ß√£o  e  regress√£o 
(perc√©ptron, modelos bayesianos, introdu√ß√£o a redes neurais, SVM, k-NN, √°rvores/florestas de 
decis√£o, etc.). Estudo da generaliza√ß√£o, medidas de erro, treinamento e teste, vi√©s e vari√¢ncia, 
overfitting,  t√©cnicas  de  regulariza√ß√£o  e  algoritmos  de  valida√ß√£o.  Estudo  do  aprendizado  n√£o-
supervisionado:  algoritmos  para  agrupamento,  detec√ß√£o  de  anomalia,  separa√ß√£o  de  sinais  e 
estima√ß√£o de densidade.

## Objetivos

Apresentar aos alunos os conceitos b√°sicos e principais caracter√≠sticas dos modelos cl√°ssicos e 
o  estado  da  arte  de  redes  neurais  artificiais,  sua  fundamenta√ß√£o  biol√≥gica  e  suas  poss√≠veis 
aplica√ß√µes em diversas √°reas com √™nfase ao projeto e constru√ß√£o de sistemas para resolu√ß√£o de 
problemas pr√°ticos.

## Conte√∫do Program√°tico

| Semana | Data | Conte√∫do | Metodologia | Recursos |
| :-: | :-: | :-- | :-: | :-: |
| 1 | 23/fev | <ul><li>Apresenta√ß√£o da disciplina</li></ul> | Ativa | Lousa e Datashow |
| 2 | 2/mar | <ul><li>O que s√£o redes neurais e como funcionam</li><li>Introdu√ß√£o ao perceptron</li><li>Como criar um modelo b√°sico de rede neural rasa usando o perceptron</li></ul> | Ativa | Lousa e Datashow |
| 3 | 9/mar | <ul><li>Treinamento de modelos: treinamento supervisionado, fun√ß√£o de perda, ajuste de hiperpar√¢metros</li></ul> | Ativa | Lousa e Datashow |
| 4 | 16/mar | <ul><li>Constru√ß√£o de modelos de redes neurais rasas com TensorFlow e Pytorch: cria√ß√£o de modelos com camadas densas e como trein√°-los</li></ul> | Ativa | Lousa e Datashow |
| 5 | 23/mar | <ul><li>Avalia√ß√£o de modelos de redes neurais rasas: m√©tricas de  avalia√ß√£o e como escolher a melhor m√©trica para o seu modelo</li><li>Carregamento de dados em TensorFlow e PyTorch: carregamento de dados de diferentes fontes (CSV, HDF5, imagens) e como preparar os dados para o treinamento de modelos em ambos os frameworks</li></ul> | Ativa | Lousa e Datashow |
| 6 | 30/mar | <ul><li>Pr√©-processamento de dados em TensorFlow e PyTorch: t√©cnicas de pr√©-processamento de dados (normaliza√ß√£o, codifica√ß√£o one-hot, etc.) e como aplic√°-las aos dados de entrada em ambos os frameworks</li><li>Modelo de rede neural rasa usando perceptron com camadas densas em PyTorch e TensorFlow: constru√ß√£o de </li><li>modelos de redes neurais rasas mais avan√ßados e como trein√°-los em ambos os frameworks</li></ul> | Ativa | Lousa e Datashow |
| 7 | 06/abr |<ul><li>Pr√©-processamento de dados em TensorFlow e PyTorch: t√©cnicas de pr√©-processamento de dados (normaliza√ß√£o, codifica√ß√£o one-hot, etc.) e como aplic√°-las aos dados de entrada em ambos os frameworks</li><li>Modelo de rede neural rasa usando perceptron com camadas densas em PyTorch e TensorFlow: constru√ß√£o de </li><li>modelos de redes neurais rasas mais avan√ßados e como trein√°-los em ambos os frameworks</li></ul> | Ativa | Lousa e Datashow |
| 8 | 13/abr | <ul><li>Primeira Avalia√ß√£o</li></ul> | Ativa | Lousa e Datashow |
| 9 | 20/abr | <ul><li>Redes Neurais Convolucionais (CNN): Introdu√ß√£o √†s redes neurais convolucionais; Arquitetura b√°sica de uma CNN: camadas convolucionais, de pooling e totalmente conectadas</li><li>Pr√©-processamento de dados para treinamento de CNNs</li><li>Constru√ß√£o de uma CNN b√°sica em TensorFlow e PyTorch</li></ul> | Ativa | Lousa e Datashow |
| 10 | 27/abr | <ul><li>Treinamento de CNNs: fun√ß√£o de perda, otimizadores e ajuste de hiperpar√¢metros</li><li>T√©cnicas de regulariza√ß√£o em CNNs: dropout, regulariza√ß√£o L1/L2</li><li>Transfer√™ncia de aprendizagem em CNNs</li><li>Implementa√ß√£o de uma CNN mais complexa em TensorFlow e PyTorch</li></ul> | Ativa | Lousa e Datashow |
| 11 | 04/mai | <ul><li>Aplica√ß√µes de CNNs em processamento de imagens: classifica√ß√£o de imagens, detec√ß√£o de objetos, segmenta√ß√£o de imagens</li><li>Trabalhando com conjuntos de dados grandes: carregamento de dados em lote, aumento de dados</li><li>Uso de ferramentas de visualiza√ß√£o para entender o comportamento da CNN</li></ul> | Ativa | Lousa e Datashow |
| 12 | 11/mai | <ul><li>Introdu√ß√£o √†s redes neurais recorrentes (RNNs): Arquitetura b√°sica de uma RNN: neur√¥nios recorrentes e camadas LSTM/GRU</li><li>Fluxo de informa√ß√£o em uma RNN</li><li>Exemplos de aplica√ß√µes de RNNs</li><li>Treinamento de RNNs: fun√ß√£o de perda, otimizadores e ajuste de hiperpar√¢metros</li><li>T√©cnicas de regulariza√ß√£o em RNNs: dropout, regulariza√ß√£o L1/L2</li><li>Implementa√ß√£o de uma RNN b√°sica em TensorFlow e/ou PyTorch</li><li>Exemplos de aplica√ß√£o de RNNs em processamento de sequ√™ncias</li></ul>  | Ativa | Lousa e Datashow |
| 13 | 18/mai | <ul><li>Aplica√ß√µes avan√ßadas de RNNs: tradu√ß√£o autom√°tica, gera√ß√£o de texto, modelagem de linguagem</li><li>Arquiteturas avan√ßadas de RNNs: RNNs bidirecionais, redes neurais de mem√≥ria de curto prazo (LSTM) e redes neurais de atualiza√ß√£o de portas (GRU)</li><li>Discuss√£o sobre os desafios de treinamento de RNNs</li><li>Implementa√ß√£o de uma RNN avan√ßada em TensorFlow ou PyTorch</li></ul> | Ativa | Lousa e Datashow |
| 14 | 25/mai | <ul><li>Introdu√ß√£o √† redes generativas advers√°rias (GANs)</li><li>Introdu√ß√£o √†s GANs: defini√ß√£o e funcionamento</li><li>Arquitetura b√°sica de uma GAN: gerador e discriminador</li><li>Fluxo de informa√ß√£o em uma GAN</li><li>Exemplos de aplica√ß√µes de GANs</li><li>Treinamento de GANs: fun√ß√£o de perda, otimizadores e ajuste de hiperpar√¢metros</li><li>T√©cnicas de regulariza√ß√£o em GANs: normaliza√ß√£o de inst√¢ncia, regulariza√ß√£o de gradiente, otimiza√ß√£o do gerador</li><li>Implementa√ß√£o de uma GAN b√°sica em TensorFlow e/ou PyTorch</li><li>Exemplos de aplica√ß√£o de GANs em gera√ß√£o de imagens</li></ul> | Ativa | Lousa e Datashow |
| 15 | 1/jun | <ul><li>Introdu√ß√£o ao aprendizado por refor√ßo: defini√ß√£o e funcionamento</li><li>Agentes e ambientes: intera√ß√£o entre o agente e o ambiente</li><li>Processo de decis√£o de Markov: estados, a√ß√µes e recompensas</li><li>Exemplos de aplica√ß√µes de aprendizado por refor√ßo</li><li>Algoritmos de aprendizado por refor√ßo: Monte Carlo, TD(0), Q-learning, SARSA</li><li>Aprendizado por refor√ßo com aproxima√ß√£o de fun√ß√£o: redes neurais como fun√ß√£o de valor ou fun√ß√£o Q</li><li>Implementa√ß√£o de um agente b√°sico em TensorFlow e/ou PyTorch</li></ul>  | Ativa | Lousa e Datashow |
| 16 | 8/jun | <ul></li><li>Corpus Christi</li></ul> | Ativa | Lousa e Datashow |
| 17 | 15/jun | <ul><li>Segunda Avalia√ß√£o</li></ul><ul> | Ativa | Lousa e Datashow |
| 18 | 22/jun | <ul><li>Fechamento das notas</li></ul> | Ativa | Lousa e Datashow |