{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizadores\n",
    "\n",
    "### Tokenizador por Espaço\n",
    "\n",
    "Desenvolvido com base no principal delimitador para uma grande parcela das línguas naturais humanas: o espaço"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto = \"No meio do caminho tinha uma pedra.\"\n",
    "texto.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizador baseado numa expressão regular\n",
    "\n",
    "Segmenta as palavras de um texto com base em delimitadores como espaço, pontuações e início/fim de uma sequência (\\b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizador baseado em Regras\n",
    "\n",
    "1. Buscar todas as ocorrências de valores numéricos e financeiros (R$1,00; $46; etc.)\n",
    "\n",
    "2. Buscar todas as ocorrências de sequências de 1 ou mais caracteres\n",
    "\n",
    "3. Buscar todas as ocorrências de sequências sem espaço\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizador baseado em Regras do NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'menino', 'jogou', 'bola', 'ontem', 'às', '16:00', '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "versos = \"\"\"O menino jogou bola ontem às 16:00.\"\"\"\n",
    "\n",
    "nltk.word_tokenize(versos, language='portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'everyone', '!', '!', '!']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Hello everyone!!!\"\"\"\n",
    "\n",
    "nltk.word_tokenize(text, language='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte-Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializando o tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializando o Módulo de Treinamento\n",
    "\n",
    "Define-se um vocabulário desejado com 30000 símbolos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], \n",
    "                     vocab_size=30000, \n",
    "                     min_frequency=0,\n",
    "                     continuing_subword_prefix=\"##\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo pré-tokenizador por espaço"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinando o tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f\"wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "tokenizer.train(files, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando o Tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tokenizer-wiki.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando o tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"tokenizer-wiki.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizando textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'don', \"'\", 't', 'go', 'out', 'ton', '##ight', '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto = \"I don't go out tonight.\"\n",
    "output = tokenizer.encode(texto)\n",
    "output.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[45, 11322, 11, 88, 9382, 9031, 14432, 8914, 18]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don ' t go out ton ##ight .\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte-level BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializando o tokenizador e o córpus de treinamento (Wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "files = [f\"wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinando o tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(files=files, vocab_size=52000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando o tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tokenizer-wiki2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizando um texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E',\n",
       " 'u',\n",
       " 'Ġest',\n",
       " 'ou',\n",
       " 'Ġna',\n",
       " 'Ġa',\n",
       " 'ula',\n",
       " 'Ġde',\n",
       " 'ĠMin',\n",
       " 'era',\n",
       " 'Ã§',\n",
       " 'Ã£o',\n",
       " 'Ġde',\n",
       " 'ĠDad',\n",
       " 'os',\n",
       " '.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tokenizer.encode(\"Eu estou na aula de Mineração de Dados.\")\n",
    "output.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import Unigram\n",
    "from tokenizers.trainers import UnigramTrainer\n",
    "\n",
    "files = [f\"wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "\n",
    "# inicializando o tokenizador\n",
    "tokenizer = Tokenizer(Unigram())\n",
    "# inicilizando o módulo de treinamento\n",
    "trainer = UnigramTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], \n",
    "                     vocab_size=30000, \n",
    "                     min_frequency=0,\n",
    "                     continuing_subword_prefix=\"##\")\n",
    "# treinando o tokenizador\n",
    "files = [f\"wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "tokenizer.train(files, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tokenizer-wiki3.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capitalização\n",
    "\n",
    "Processo de colocar os tokens em letra minúscula para normalização do texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no',\n",
       " 'meio',\n",
       " 'do',\n",
       " 'caminho',\n",
       " 'tinha',\n",
       " 'uma',\n",
       " 'pedra',\n",
       " 'tinha',\n",
       " 'uma',\n",
       " 'pedra',\n",
       " 'no',\n",
       " 'meio',\n",
       " 'do',\n",
       " 'caminho']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "versos = \"\"\"No meio do caminho tinha uma pedra\n",
    "Tinha uma pedra no meio do caminho\"\"\".lower()\n",
    "\n",
    "nltk.word_tokenize(versos, language='portuguese')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenização de Sentenças"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Eu estou na aula de Mineração de Dados.', 'Os estudantes são muito bons.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "texto = 'Eu estou na aula de Mineração de Dados. Os estudantes são muito bons.'\n",
    "\n",
    "nltk.sent_tokenize(texto, language='portuguese')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lematização\n",
    "\n",
    "Instalando o Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (3.3.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (8.0.17)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (62.5.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (0.7.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (2.4.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (1.0.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (1.22.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (0.9.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from packaging>=20.0->spacy) (3.0.8)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (4.2.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.2.2\n",
      "[notice] To update, run: C:\\Users\\RooneyCoelho\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "Collecting pt-core-news-lg==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-3.3.0/pt_core_news_lg-3.3.0-py3-none-any.whl (568.2 MB)\n",
      "     -------------------------------------- 568.2/568.2 MB 1.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pt-core-news-lg==3.3.0) (3.3.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (0.6.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (8.0.17)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (21.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (2.27.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (3.1.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (4.64.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (62.5.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (1.22.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (3.0.8)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (4.2.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (1.26.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rooneycoelho\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->pt-core-news-lg==3.3.0) (2.1.1)\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.2.2\n",
      "[notice] To update, run: C:\\Users\\RooneyCoelho\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('pt_core_news_lg')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python3 -m spacy download pt_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"pt_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O o\n",
      "passado passado\n",
      "é ser\n",
      "só só\n",
      "uma um\n",
      "história história\n",
      "que que\n",
      "nos nós\n",
      "contamos contar\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"O passado é só uma história que nos contamos.\")\n",
    "\n",
    "for token in doc:\n",
    "  print(token, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radicalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'com', 'est', 'gost']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('rslp')\n",
    "\n",
    "raiz = nltk.stem.RSLPStemmer()\n",
    "\n",
    "tokens = nltk.word_tokenize('A comida estava gostosa', language='portuguese')\n",
    "[raiz.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de Linguagem e N-gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\RooneyCoelho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 1: Carregando o Córpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no meio do caminho tinha uma pedra',\n",
       " 'tinha uma pedra no meio do caminho',\n",
       " 'tinha uma pedra',\n",
       " 'no meio do caminho tinha uma pedra']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto = \"\"\"No meio do caminho tinha uma pedra\n",
    "Tinha uma pedra no meio do caminho\n",
    "Tinha uma pedra\n",
    "No meio do caminho tinha uma pedra\"\"\"\n",
    "\n",
    "texto = texto.lower().split('\\n')\n",
    "texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 2: Tokenizando as Sentenças do Córpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra'],\n",
       " ['tinha', 'uma', 'pedra', 'no', 'meio', 'do', 'caminho'],\n",
       " ['tinha', 'uma', 'pedra'],\n",
       " ['no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto_tok = []\n",
    "for verso in texto:\n",
    "  tokens = nltk.word_tokenize(verso, language='portuguese')\n",
    "  texto_tok.append(tokens)\n",
    "\n",
    "texto_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 3: Inserindo Marcadores de Início e Fim de Sentença\n",
    "Suponha que queiramos definir um modelo de linguagem com bigramas, ou seja, calcular as chances de uma palavra com base na anterior (e.g., $P(pedra | uma)$, temos que marcar o início e fim da sentença para poder prever as changes da primeira palavra (e.g., $P(no | \\langle s \\rangle)$) e o fim da sentença ((e.g., $P(\\langle/ s \\rangle)$ | pedra)). Este processo é conhecido como *padding*.\n",
    "\n",
    "Podemos fazer o *padding* de uma sentença utilizando o método **nltk.lm.preprocessing.pad_both_ends**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>', 'no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra', '</s>'],\n",
       " ['<s>', 'tinha', 'uma', 'pedra', 'no', 'meio', 'do', 'caminho', '</s>'],\n",
       " ['<s>', 'tinha', 'uma', 'pedra', '</s>'],\n",
       " ['<s>', 'no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra', '</s>']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "\n",
    "ngramas = 2 # definindo o número de n-gramas (no caso, 2 -> bigramas)\n",
    "\n",
    "texto_tok_pad = []\n",
    "for verso in texto_tok:\n",
    "  padded = pad_both_ends(verso, n=ngramas)\n",
    "  texto_tok_pad.append(list(padded))\n",
    "\n",
    "texto_tok_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 4: Calculando os N-Gramas\n",
    "\n",
    "Uma vez que as sentenças do nosso córpus foram pré-processadas, podemos calcular os n-gramas (neste caso, os bigramas), utilizando o método **nltk.ngrams**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('<s>', 'no'),\n",
       "  ('no', 'meio'),\n",
       "  ('meio', 'do'),\n",
       "  ('do', 'caminho'),\n",
       "  ('caminho', 'tinha'),\n",
       "  ('tinha', 'uma'),\n",
       "  ('uma', 'pedra'),\n",
       "  ('pedra', '</s>')],\n",
       " [('<s>', 'tinha'),\n",
       "  ('tinha', 'uma'),\n",
       "  ('uma', 'pedra'),\n",
       "  ('pedra', 'no'),\n",
       "  ('no', 'meio'),\n",
       "  ('meio', 'do'),\n",
       "  ('do', 'caminho'),\n",
       "  ('caminho', '</s>')],\n",
       " [('<s>', 'tinha'), ('tinha', 'uma'), ('uma', 'pedra'), ('pedra', '</s>')],\n",
       " [('<s>', 'no'),\n",
       "  ('no', 'meio'),\n",
       "  ('meio', 'do'),\n",
       "  ('do', 'caminho'),\n",
       "  ('caminho', 'tinha'),\n",
       "  ('tinha', 'uma'),\n",
       "  ('uma', 'pedra'),\n",
       "  ('pedra', '</s>')]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngramas = 2\n",
    "\n",
    "bigramas_pad = []\n",
    "for verso in texto_tok_pad:\n",
    "  bigramas = nltk.ngrams(verso, ngramas)\n",
    "  bigramas_pad.append(list(bigramas))\n",
    "\n",
    "bigramas_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contudo, para deixar nosso modelo de linguagem mais robusto, vamos calcular os **unigramas** além dos **bigramas** utilizando o comando **nltk.util.everygrams**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('<s>',),\n",
       "  ('<s>', 'no'),\n",
       "  ('no',),\n",
       "  ('no', 'meio'),\n",
       "  ('meio',),\n",
       "  ('meio', 'do'),\n",
       "  ('do',),\n",
       "  ('do', 'caminho'),\n",
       "  ('caminho',),\n",
       "  ('caminho', 'tinha'),\n",
       "  ('tinha',),\n",
       "  ('tinha', 'uma'),\n",
       "  ('uma',),\n",
       "  ('uma', 'pedra'),\n",
       "  ('pedra',),\n",
       "  ('pedra', '</s>'),\n",
       "  ('</s>',)],\n",
       " [('<s>',),\n",
       "  ('<s>', 'tinha'),\n",
       "  ('tinha',),\n",
       "  ('tinha', 'uma'),\n",
       "  ('uma',),\n",
       "  ('uma', 'pedra'),\n",
       "  ('pedra',),\n",
       "  ('pedra', 'no'),\n",
       "  ('no',),\n",
       "  ('no', 'meio'),\n",
       "  ('meio',),\n",
       "  ('meio', 'do'),\n",
       "  ('do',),\n",
       "  ('do', 'caminho'),\n",
       "  ('caminho',),\n",
       "  ('caminho', '</s>'),\n",
       "  ('</s>',)],\n",
       " [('<s>',),\n",
       "  ('<s>', 'tinha'),\n",
       "  ('tinha',),\n",
       "  ('tinha', 'uma'),\n",
       "  ('uma',),\n",
       "  ('uma', 'pedra'),\n",
       "  ('pedra',),\n",
       "  ('pedra', '</s>'),\n",
       "  ('</s>',)],\n",
       " [('<s>',),\n",
       "  ('<s>', 'no'),\n",
       "  ('no',),\n",
       "  ('no', 'meio'),\n",
       "  ('meio',),\n",
       "  ('meio', 'do'),\n",
       "  ('do',),\n",
       "  ('do', 'caminho'),\n",
       "  ('caminho',),\n",
       "  ('caminho', 'tinha'),\n",
       "  ('tinha',),\n",
       "  ('tinha', 'uma'),\n",
       "  ('uma',),\n",
       "  ('uma', 'pedra'),\n",
       "  ('pedra',),\n",
       "  ('pedra', '</s>'),\n",
       "  ('</s>',)]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import everygrams\n",
    "ngramas = 2\n",
    "\n",
    "ngramas_pad = []\n",
    "for verso in texto_tok_pad:\n",
    "  bigramas = everygrams(verso, max_len=ngramas)\n",
    "  ngramas_pad.append(list(bigramas))\n",
    "\n",
    "ngramas_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 5: Colocando todos os tokens do córpus numa única lista\n",
    "**nltk.lm.preprocessing.flatten**:\n",
    "\n",
    "Este método converte junta os elementos de sublistas em uma única lista. Por exemplo:\n",
    "\n",
    "```python\n",
    ">>> lista = [[1, 2], [3, 4]]\n",
    ">>> flatten(lista)\n",
    "[1, 2, 3, 4]\n",
    "```\n",
    "\n",
    "Como pode ser visto abaixo, nós o utilizamos para juntar todas os tokens das sentenças de nosso corpus numa única lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>', 'no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra', '</s>'],\n",
       " ['<s>', 'tinha', 'uma', 'pedra', 'no', 'meio', 'do', 'caminho', '</s>'],\n",
       " ['<s>', 'tinha', 'uma', 'pedra', '</s>'],\n",
       " ['<s>', 'no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra', '</s>']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto_tok_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'no',\n",
       " 'meio',\n",
       " 'do',\n",
       " 'caminho',\n",
       " 'tinha',\n",
       " 'uma',\n",
       " 'pedra',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'tinha',\n",
       " 'uma',\n",
       " 'pedra',\n",
       " 'no',\n",
       " 'meio',\n",
       " 'do',\n",
       " 'caminho',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'tinha',\n",
       " 'uma',\n",
       " 'pedra',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'no',\n",
       " 'meio',\n",
       " 'do',\n",
       " 'caminho',\n",
       " 'tinha',\n",
       " 'uma',\n",
       " 'pedra',\n",
       " '</s>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import flatten\n",
    "\n",
    "tokens = list(flatten(texto_tok_pad)) # juntando as palavras do nosso córpus\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 6: Definindo o Vocabulário\n",
    "\n",
    "**nltk.lm.Vocabulary**\n",
    "\n",
    "Utilizado para definir o vocabulário do nosso córpus. Recebe dois parâmetros como entrada: uma lista com todos os tokens do nosso córpus e a variável *unk_cutoff*, a qual passa a considerar palavras abaixo de um limiar de frequência como palavras fora do vocabuário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import Vocabulary\n",
    "\n",
    "vocab = Vocabulary(tokens, unk_cutoff=1) # definindo o vocabulário do nosso córpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtendo as frequências das palavras do córpus com o comando *counts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'<s>': 4,\n",
       "         'no': 3,\n",
       "         'meio': 3,\n",
       "         'do': 3,\n",
       "         'caminho': 3,\n",
       "         'tinha': 4,\n",
       "         'uma': 4,\n",
       "         'pedra': 4,\n",
       "         '</s>': 4})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "procurando uma palavra no vocabulário. Caso não encontrada, o token de palavra fora do vocabulário será retornada (\\<UNK>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tinha', '<UNK>')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup(\"tinha\"), vocab.lookup(\"homem\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplificando o Pré-processamento\n",
    "\n",
    "Agora que você sabe cada passo do pré-processamento (inserir marcadores de início e fim de sentença, calcular os n-gramas, juntar todos os tokens do corpus numa lista e definir o vocabulário), este processo pode ser simplificado pela funcionalidade **nltk.lm.preprocessing.padded_everygram_pipeline**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('<s>',),\n",
       "  ('<s>', 'no'),\n",
       "  ('no',),\n",
       "  ('no', 'meio'),\n",
       "  ('meio',),\n",
       "  ('meio', 'do'),\n",
       "  ('do',),\n",
       "  ('do', 'caminho'),\n",
       "  ('caminho',),\n",
       "  ('caminho', 'tinha'),\n",
       "  ('tinha',),\n",
       "  ('tinha', 'uma'),\n",
       "  ('uma',),\n",
       "  ('uma', 'pedra'),\n",
       "  ('pedra',),\n",
       "  ('pedra', '</s>'),\n",
       "  ('</s>',)],\n",
       " [('<s>',),\n",
       "  ('<s>', 'tinha'),\n",
       "  ('tinha',),\n",
       "  ('tinha', 'uma'),\n",
       "  ('uma',),\n",
       "  ('uma', 'pedra'),\n",
       "  ('pedra',),\n",
       "  ('pedra', 'no'),\n",
       "  ('no',),\n",
       "  ('no', 'meio'),\n",
       "  ('meio',),\n",
       "  ('meio', 'do'),\n",
       "  ('do',),\n",
       "  ('do', 'caminho'),\n",
       "  ('caminho',),\n",
       "  ('caminho', '</s>'),\n",
       "  ('</s>',)],\n",
       " [('<s>',),\n",
       "  ('<s>', 'tinha'),\n",
       "  ('tinha',),\n",
       "  ('tinha', 'uma'),\n",
       "  ('uma',),\n",
       "  ('uma', 'pedra'),\n",
       "  ('pedra',),\n",
       "  ('pedra', '</s>'),\n",
       "  ('</s>',)],\n",
       " [('<s>',),\n",
       "  ('<s>', 'no'),\n",
       "  ('no',),\n",
       "  ('no', 'meio'),\n",
       "  ('meio',),\n",
       "  ('meio', 'do'),\n",
       "  ('do',),\n",
       "  ('do', 'caminho'),\n",
       "  ('caminho',),\n",
       "  ('caminho', 'tinha'),\n",
       "  ('tinha',),\n",
       "  ('tinha', 'uma'),\n",
       "  ('uma',),\n",
       "  ('uma', 'pedra'),\n",
       "  ('pedra',),\n",
       "  ('pedra', '</s>'),\n",
       "  ('</s>',)]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "ngramas = 2\n",
    "\n",
    "ngramas_pad, vocab = padded_everygram_pipeline(ngramas, texto_tok)\n",
    "\n",
    "ngramas_pad = [list(w) for w in ngramas_pad]\n",
    "ngramas_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 7: Treinando um modelo de linguagem\n",
    "\n",
    "Um modelo de linguagem pode ser treinado utilizando a funcionalidade **nltk.lm.MLE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline, flatten\n",
    "from nltk.lm import MLE\n",
    "\n",
    "ngramas = 2\n",
    "ngramas_pad, vocab = padded_everygram_pipeline(ngramas, texto_tok)\n",
    "lm = MLE(ngramas)\n",
    "lm.fit(ngramas_pad, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado o token **\\<s>**, gerando um texto de 4 tokens com o modelo de linguagem treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no', 'meio', 'do', 'caminho']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.generate(4, text_seed=[\"<s>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilidade da palavra *no*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidade comum:  0.09375\n",
      "Probabilidade logarítmica:  -3.415037499278844\n"
     ]
    }
   ],
   "source": [
    "print(\"Probabilidade comum: \", lm.score(\"no\"))\n",
    "print(\"Probabilidade logarítmica: \", lm.logscore(\"no\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilidade da palavra *tinha* dado a palavra *caminho*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidade comum:  0.6666666666666666\n",
      "Probabilidade logarítmica:  -0.5849625007211563\n"
     ]
    }
   ],
   "source": [
    "print(\"Probabilidade comum: \", lm.score(\"tinha\", context=[\"caminho\"]))\n",
    "print(\"Probabilidade logarítmica: \", lm.logscore(\"tinha\", context=[\"caminho\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representações Vetoriais para Palavras, Sentenças e Documentos\n",
    "\n",
    "### Similaridade por Cossenos\n",
    "\n",
    "Normalmente em PLN, a distância entre dois vetores é calculada através da similaridade por cosseno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7071067811865475"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "po = np.array([[5, 10]])\n",
    "mestre_tigresa = np.array([[7.5, 2.5]])\n",
    "\n",
    "cosine_similarity(po, mestre_tigresa)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representação *One-Hot*\n",
    "\n",
    "Palavras e documentos são representados por vetores de dimensão do tamanho do vocabulário. Os vetores assumem valores binários (0 ou 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário:  ['caminho', 'do', 'meio', 'no', 'pedra', 'tinha', 'uma']\n",
      "Vetores\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "X = [[\"no\"], [\"meio\"], [\"do\"], [\"caminho\"], [\"tinha\"], [\"uma\"], [\"pedra\"]]\n",
    "\n",
    "enc.fit(X)\n",
    "vocab = list(enc.categories_[0])\n",
    "vetores = enc.transform(X).toarray()\n",
    "\n",
    "print('Vocabulário: ', vocab)\n",
    "print('Vetores')\n",
    "vetores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vetor One-Hot de *pedra*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vetores[vocab.index('pedra')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz de Frequência Termo-Documento\n",
    "\n",
    "Dado um vocabulário e um conjunto de documentos, as representações das palavras e dos documentos podem ser calculadas a partir da contagem de cada palavra em cada documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário\n",
      "['caminho' 'do' 'meio' 'no' 'pedra' 'tinha' 'uma']\n",
      "\n",
      "Matrix\n",
      "[[1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1]\n",
      " [0 0 0 0 1 1 1]\n",
      " [1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['no meio do caminho tinha uma pedra',\n",
    " 'tinha uma pedra no meio do caminho',\n",
    " 'tinha uma pedra',\n",
    " 'no meio do caminho tinha uma pedra']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vetores = vectorizer.fit_transform(corpus)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "print('Vocabulário')\n",
    "print(vocab)\n",
    "print()\n",
    "print('Matrix')\n",
    "print(vetores.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz Termo-Termo\n",
    "\n",
    "Dado um vocabulário, a representação de uma palavra pode ser calculada a partir da contagem de sua co-ocorrência com cada palavra do vocabulário num determinado contexto (e.g. documento, sentença, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário\n",
      "['no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra']\n",
      "\n",
      "Matrix\n",
      "[[0. 3. 3. 3. 3. 3. 3.]\n",
      " [3. 0. 3. 3. 3. 3. 3.]\n",
      " [3. 3. 0. 3. 3. 3. 3.]\n",
      " [3. 3. 3. 0. 3. 3. 3.]\n",
      " [3. 3. 3. 3. 0. 4. 4.]\n",
      " [3. 3. 3. 3. 4. 0. 4.]\n",
      " [3. 3. 3. 3. 4. 4. 0.]]\n"
     ]
    }
   ],
   "source": [
    "corpus = ['no meio do caminho tinha uma pedra',\n",
    " 'tinha uma pedra no meio do caminho',\n",
    " 'tinha uma pedra',\n",
    " 'no meio do caminho tinha uma pedra']\n",
    "\n",
    "corpus_tok = [verso.split() for verso in corpus]\n",
    "\n",
    "vocab = [\"no\", \"meio\", \"do\", \"caminho\", \"tinha\", \"uma\", \"pedra\"]\n",
    "vetores = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "for verso in corpus_tok:\n",
    "  for i, w1 in enumerate(vocab):\n",
    "    for j, w2 in enumerate(vocab):\n",
    "      if i != j:\n",
    "        if w1 in verso and w2 in verso:\n",
    "          vetores[i, j] += 1\n",
    "\n",
    "print('Vocabulário')\n",
    "print(vocab)\n",
    "print()\n",
    "print('Matrix')\n",
    "print(vetores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remoção de Palavras Vazias\n",
    "\n",
    "Palavras vazias (e.g., artigos, preposições, etc.), que possuem alta frequência em todos os documentos, podem ser removidas da contagem para melhorar a distinção entre documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário\n",
      "['caminho' 'meio' 'pedra']\n",
      "\n",
      "Matrix\n",
      "[[1 1 1]\n",
      " [1 1 1]\n",
      " [0 0 1]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['no meio do caminho tinha uma pedra',\n",
    " 'tinha uma pedra no meio do caminho',\n",
    " 'tinha uma pedra',\n",
    " 'no meio do caminho tinha uma pedra']\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stopwords)\n",
    "\n",
    "vetores = vectorizer.fit_transform(corpus)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "print('Vocabulário')\n",
    "print(vocab)\n",
    "print()\n",
    "print('Matrix')\n",
    "print(vetores.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário\n",
      "['ainda' 'entenda' 'mal' 'pergunte' 'que' 'repitas' 'respondas' 'te']\n",
      "\n",
      "Matrix\n",
      "[[0.39 0.   0.39 0.74 0.39 0.   0.   0.  ]\n",
      " [0.39 0.   0.39 0.   0.39 0.   0.74 0.  ]\n",
      " [0.31 0.6  0.31 0.   0.31 0.   0.   0.6 ]\n",
      " [0.39 0.   0.39 0.   0.39 0.74 0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "corpus = ['ainda que mal pergunte',\n",
    " 'ainda que mal respondas',\n",
    " 'ainda que mal te entenda',\n",
    " 'ainda que mal repitas']\n",
    "\n",
    "vectorizer = Pipeline([('count', CountVectorizer()),\n",
    "                 ('tfid', TfidfTransformer())])\n",
    "\n",
    "vetores = vectorizer.fit_transform(corpus)\n",
    "vocab = vectorizer['count'].get_feature_names_out()\n",
    "\n",
    "print('Vocabulário')\n",
    "print(vocab)\n",
    "print()\n",
    "print('Matrix')\n",
    "print(np.round(vetores.toarray(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acessando o primeiro (*ainda que mal pergunte*) e terceiro (*ainda que mal te entenda*) versos e calculando a similaridade entre eles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3611073242896012"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verso1 = vetores[0, :]\n",
    "verso3 = vetores[2, :]\n",
    "\n",
    "cosine_similarity(verso1, verso3)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://143.107.183.175:22980/download.php?file=embeddings/word2vec/cbow_s50.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load_word2vec_format('cbow_s50.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acessando o word embedding da palavra *menino*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.047754, -0.190243,  0.290581,  0.035822,  0.2301  , -0.139099,\n",
       "       -0.232351, -0.119084,  0.327645,  0.160017, -0.5318  ,  0.093309,\n",
       "       -0.545777, -0.166715,  0.044872, -0.094386, -0.017529, -0.053898,\n",
       "        0.189092, -0.233779, -0.302459,  0.707696, -0.146762,  0.258651,\n",
       "        0.25436 , -0.071892,  0.132296, -0.072721,  0.162642,  0.348834,\n",
       "        0.129191, -0.030967,  0.048024,  0.26683 , -0.076066,  0.352168,\n",
       "        0.629779, -0.403468, -0.473612,  0.456509,  0.008285,  0.066872,\n",
       "        0.082632, -0.128989,  0.107645,  0.119981,  0.219388, -0.141599,\n",
       "       -0.20074 , -0.30657 ], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec['menino']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Palavras mais semelhantes ao verbo *estudar*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pesquisar', 0.8674625158309937),\n",
       " ('ensinar', 0.8485607504844666),\n",
       " ('leccionar', 0.8399008512496948),\n",
       " ('moldar', 0.8285380601882935),\n",
       " ('desenvolver', 0.8203483819961548),\n",
       " ('focalizar', 0.8188527226448059),\n",
       " ('cursar', 0.8175848126411438),\n",
       " ('projectar', 0.8173723816871643),\n",
       " ('desenhar', 0.8155599236488342),\n",
       " ('enriquecer', 0.8142802715301514)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar('estudar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similaridade por cosseno entre os word embeddings das palavras *menino* e *cachorro*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8441181"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.similarity('menino', 'cachorro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferência lógica para: *odiar* está para *odiando*, assim como *amar* está para..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amando', 0.7472066283226013),\n",
       " ('desperto', 0.7231095433235168),\n",
       " ('quieto', 0.6835169196128845),\n",
       " ('tranqüilo', 0.6812532544136047),\n",
       " ('surdo', 0.6798273921012878),\n",
       " ('louco', 0.6784767508506775),\n",
       " ('quieta', 0.6757060885429382),\n",
       " ('sã³brio', 0.6748781204223633),\n",
       " ('rouco', 0.6719405651092529),\n",
       " ('sossegado', 0.6716687679290771)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar(positive=['amar', 'odiando'], negative=['odiar'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19f8edf9f3d47184070387e5526ef4245906fb1893b7f74c12e591a863440618"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
