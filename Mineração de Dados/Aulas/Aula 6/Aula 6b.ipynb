{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "O teorema de Bayes é o principal método para entender a probabilidade de algum evento $P(A|B)$, dada alguma informação nova, $P(B|A)$, e uma crença prévia na probabilidade do evento, $P(A)$:\n",
    "$$\n",
    "P(A | B) = \\frac{P(B|A)P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "Em aprendizado de máquina, uma aplicação do teorema de Bayes para classificar vem na forma do classificador Bayes ingênuo. Os classificadores Naive Bayes combinam várias qualidades desejáveis ​​em aprendizado de máquina prático em um único classificador:\n",
    "\n",
    "1. Uma abordagem intuitiva\n",
    "2. A capacidade de trabalhar com dados pequenos\n",
    "3. Baixos custos de computação para treinamento e previsão\n",
    "4. Muitas vezes, resultados sólidos em uma variedade de configurações\n",
    "\n",
    "Especificamente, um classificador naive bayes é baseado em:\n",
    "$$\n",
    "P(y | x_1, ..., x_j) = \\frac{P(x_1, ..., x_j | y)P(y)}{P(x_1,...,x_j)}\n",
    "$$\n",
    "Onde,\n",
    "* $P(y | x_1, ..., x_j)$ é chamado de *posterior* e é a probabilidade de que uma observação seja de classe y dados os valores da observação para as j features, $x_1, ..., x_j$\n",
    "* $P(x_1, ..., x_j)$ é chamado de probabilidade e é a *probabilidade* dos valores de uma observação para características, $x_1, ..., x_j$, dada sua classe y.\n",
    "* $P(y)$ é chamado de *anterior* e é nossa crença para a probabilidade da classe y antes de analisar os dados\n",
    "* P($x_1, ..., x_j$) é chamado de *probabilidade marginal*\n",
    "\n",
    "Em Naive Bayes, comparamos os valores a posteriori de uma observação para cada classe possível. Especificamente, como a probabilidade marginal é constante nessas comparações, comparamos os numeradores da posterior para cada classe. Para cada observação, a classe com o maior numerador posterior torna-se a classe prevista, $\\hat y$.\n",
    "\n",
    "Há duas coisas importantes a serem observadas sobre os classificadores Naive Bayes.\n",
    "\n",
    "1. para cada característica nos dados, temos que assumir a distribuição estatística de verossimilhança, $P(x_1, ..., x_j)$.\n",
    "- as distribuições comuns são as distribuições normal (Gaussiana), multinomial e de Bernoulli.\n",
    "- a distribuição escolhida é frequentemente determinada pela natureza dos recursos (contínuo, binário, etc.)\n",
    "\n",
    "2. Naive Bayes recebe esse nome porque assumimos que cada recurso e sua probabilidade resultante são independentes. Essa suposição \"ingênua\" é frequentemente errada, mas na prática não é suficiente para evitar a construção de classificadores de alta qualidade.\n",
    "\n",
    "## Treinando um classificador para recursos contínuos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O tipo mais comum de classificador Naive Bayes é o Gaussiano. Em Naive Bayes Gaussiano, assumimos que a probabilidade dos valores de característica, x, dada uma observação ser de classe y, segue uma distribuição normal:\n",
    "$$\n",
    "p(x_j | y) = \\frac{1}{\\sqrt{2\\pi \\sigma_y^2}} e^{-\\frac{(x_j - \\mu_y)^2}{2\\sigma_y^2}}\n",
    "$$\n",
    "onde $\\sigma_y^2$ e $\\mu_y$ são a variância e os valores médios do recurso x_j para a classe y. Por causa da suposição da distribuição normal,  Naive Bayes é melhor usado nos casos em que todas as nossas características são contínuas.\n",
    "\n",
    "Um dos aspectos interessantes dos classificadores Naive Bayes é que eles nos permitem atribuir uma crença prévia sobre as classes alvo. Podemos fazer isso usando o parâmetro `priors` do `GaussianNB`, que recebe uma lista das probabilidades atribuídas a cada classe do vetor alvo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "classifier = GaussianNB()\n",
    "\n",
    "model = classifier.fit(features, target)\n",
    "\n",
    "new_observation = [[4, 4, 4, 0.4]]\n",
    "model.predict(new_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando um classificador para recursos discretos e de contagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text_data = np.array(['I love Brazil. Brazil!', 'Brazil is best', 'Germany beats both'])\n",
    "\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "\n",
    "features = bag_of_words.toarray()\n",
    "\n",
    "target = np.array([0, 0, 1])\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "model = classifier.fit(features, target)\n",
    "\n",
    "new_observation = [[0, 0, 0, 1, 0, 1, 0]]\n",
    "model.predict(new_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Naive Bayes Multinomial funciona de maneira semelhante ao Naive Bayes Gaussiano, mas os recursos são assumidos como distribuídos multinomial. Na prática, isso significa que esse classificador é comumente usado quando temos dados discretos. Um dos usos mais comuns é a classificação de texto usando o saco de palavras ou abordagens tf-idf\n",
    "\n",
    "## Treinando um classificador Naive Bayes para recursos binários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "features = np.random.randint(2, size=(100, 3))\n",
    "target = np.random.randint(2, size=(100, 1)).ravel()\n",
    "\n",
    "classifier = BernoulliNB()\n",
    "model = classifier.fit(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O classificador Naive Bayes de Bernoulli assume que todos os nossos recursos são binários, de modo que eles assumem apenas dois valores (por exemplo, um recurso categórico nominal que foi codificado com one hot). Como seu primo multinomial, Bernoulli naive Bayes é frequentemente usado na classificação de texto, quando nossa matriz de atributos é simplesmente a presença ou ausência de uma palavra em um documento"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "5fe8ef569c9c2a7d6f6f4604c3dd3e325cd38c6a3a4c920304da28dd78ba0144"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
